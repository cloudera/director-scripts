#
# Copyright (c) 2016 Cloudera, Inc. All rights reserved.
#

#
# Sample Cloudera Director configuration file based on the Cloudera Azure reference architecture:
# http://www.cloudera.com/documentation/other/reference-architecture/PDF/cloudera_ref_arch_azure.pdf
#
# This will create a Highly Available cluster with 3 master nodes and 5 worker nodes, with Kerberos enabled
#


#
# Cluster name
# If environmentName and deploymentName are not defined they will get the value of 'name'.
# Must be unique.
#

name: C5-Kerberos-Azure

#
# Environment name
#

environmentName: Azure-Kerberos-HA

#
# Deployment name
# Used to name the Cloudera Manager instance in Cloudera Director.
# Must be unique.
#

deploymentName: Cloudera-Manager-on-Azure


#
# Cloud provider configuration (credentials, region, and management/authentication endpoints)
#

provider {
    type: azure

    #
    # ID of Azure region to use. NOTE: region must support Premium Storage
    # See: https://azure.microsoft.com/en-us/regions/#services
    #

    region: "region_REPLACE_ME"

    #
    # Azure Resource Management URL.
    #

    mgmtUrl: "https://management.core.windows.net/"

    #
    # Azure Active Directory Subscription ID.
    #

    subscriptionId: "subscriptionId_REPLACE_ME"

    #
    # Azure Active Directory URL.
    #

    aadUrl: "https://login.windows.net/"

    #
    # Tenant ID (from AAD)
    #

    tenantId: "tenantId_REPLACE_ME"

    #
    # Azure Active Directory Application Client ID.
    #

    clientId: "clientId_REPLACE_ME"

    #
    # Client Secret
    #

    clientSecret: "clientSecret_REPLACE_ME"
}


#
# SSH credentials to use to connect to the machines
#

ssh {
    username: "username_REPLACE_ME"
    privateKey: """-----BEGIN RSA PRIVATE KEY-----
privateKey_REPLACE_ME
-----END RSA PRIVATE KEY-----"""
}


#
# Common variable definitions
#
# These are the key / value pairs used to define instance templates. They are defined here and
# referenced throughout this config file.
# More about HOCON substitution: https://github.com/typesafehub/config/blob/master/HOCON.md#substitutions
#
# The format of this section:
#   - base: represents the core config fields that are common to all nodes
#   - master-base: represents the core config fields that are common to all master nodes
#   - master-1: represents the core config fields that are common to master-1 nodes
#   - master-2: represents the core config fields that are common to master-2 nodes
#   - worker: represents the core config fields that are common to worker nodes
#   - edge: represents the core config fields that are common to edge nodes
#
#
# Instance Template Breakdown
# An instance template configuration consists of the following fields. Unless otherwise specified,
# all fields are required.
#
# - image: The image ID used for instances is an alias defined in the plugin configuration file.
#
# - type: The VM type. See the Azure RA for more detail.
#
# - computeResourceGroup: Resource Group for the deployment.  The Resource Group you specify must
#   exist within the region you selected.
#   See: https://azure.microsoft.com/en-us/documentation/articles/resource-group-overview/
#
# - networkSecurityGroupResourceGroup: The Resource Group for the Network Security Group. The
#   Resource Group you specify must exist within the region you selected.
#   See: https://azure.microsoft.com/en-us/documentation/articles/resource-group-overview/
#
# - networkSecurityGroup: The Network Security Group for this instance type, this has to be
#   within the networkSecurityGroupResourceGroup. NSG configuration allows you to limit access to
#   the VM with firewall-like rules.
#   See: https://azure.microsoft.com/en-us/documentation/articles/virtual-networks-nsg/
#
# - virtualNetworkResourceGroup: The Resource Group for the Virtual Network. The Resource Group you
#   specify must exist within the region you selected and should be the same for all instances that
#   will be used in the same cluster.
#   See: https://azure.microsoft.com/en-us/documentation/articles/resource-group-overview/
#
# - virtualNetwork: The Azure Virtual Network that will be used, this has to be within the
#   virtualNetworkResourceGroup and should be the same for all instances that will be used in the
#   same cluster.
#   See: https://azure.microsoft.com/en-us/documentation/services/virtual-network/
#
# - subnetName: The name of the Subnet that will be used, this has to be within the virtualNetwork.
#
# - instanceNamePrefix: Prefix for VM name and hostname of the VM. The VM name will have the
#   folloing format:
#       instanceNamePrefix-{UUID}
#   where {UUID} is generated by the Cloudera Director server.
#
# - hostFqdnSuffix: Hostname FQDN Suffix. This is the DNS domain you configured in your custom DNS
#   server. Example values are: cdh-cluster.internal, cluster.your-company-name.com. The host FQDN
#   is configured on the VMs with the following format:
#       {instanceNamePrefix}-{truncated-UUID}.hostFqdnSuffix
#
# - availabilitySet: Availability Set for this instance type.  Machines within the same availability
#   set will have staggared maintanance times. With a default availability set configuration no more
#   than 1/5 machines will be offline at a time (rounded up).
#   See: https://azure.microsoft.com/en-us/documentation/articles/virtual-machines-linux-manage-availability/
#   Sharing Availability Set between master and worker nodes is strongly not recommended.
#
# - publicIP: Should this instance type have Azure Public IP Address and DNS Label?  If Yes, the
#   machine will have a publically resolvable hostname with the folling format:
#       {instanceNamePrefix}-{UUID}.{region}.coudapp.azure.com
#   Allowed values: Yes, No
#
# - storageAccountType: The storage account type to use. The dataDiskSize parameter should be
#   updated based on the storage account type used.
#   The current allowed values are:
#       - PremiumLRS
#       - StandardLRS
#   See the RA for the supported ways to use standard storage:
#   http://www.cloudera.com/documentation/other/reference-architecture/PDF/cloudera_ref_arch_azure.pdf
#
# - dataDiskCount: The number of data drives. The size can be specified with `dataDiskSize`
#   Data drives are mounted on /data0 .. /data[n]
#       /data0 - Dedicated Log Device
#   For Masters-1:
#       /data1 - HDFS JournalNode Data
#       /data2 - Zookeeper Data / DataLog
#       /data3 - NameNode Data
#   For Masters-2:
#       /data1 - HDFS JournalNode Data
#       /data2 - Zookeeper Data / DataLog
#   For Workers:
#       /data1 .. /data[n] will be used for HDFS data
#
# - dataDiskSize: The size of each data drive.
#   For disks allocated in a premium storage account, only the following GB values are allowed:
#       512 (P20 disk)
#       1023 (P30 disk) (1023, not 1024)
#   For disks alllocated in a standard storage account, any size between 1 and 1023 inclusive can
#   be used.
#   See https://azure.microsoft.com/en-us/documentation/articles/storage-introduction/ and
#   https://azure.microsoft.com/en-us/documentation/articles/storage-premium-storage/
#
# - tags (optional): Additional tags to help label resources within Azure
#
# - bootstrapScript: The bootstrap script (see the bootstrap-script section)
#

common-instanceTemplate {

    # Core config fields that are common to all node types
    base {
        type: STANDARD_DS14
        image: cloudera-centos-6-latest
        networkSecurityGroupResourceGroup: "networkSecurityGroupResourceGroup_REPLACE_ME"
        networkSecurityGroup: "networkSecurityGroup_REPLACE_ME"
        virtualNetworkResourceGroup: "virtualNetworkResourceGroup_REPLACE_ME"
        virtualNetwork: "virtualNetwork_REPLACE_ME"
        subnetName: "subnetName_REPLACE_ME"
        hostFqdnSuffix: "hostFqdnSuffix_REPLACE_ME"
        tags {
            owner: ${?USER}
        }
    }

    # Core config fields that are common to all master nodes
    master-base {
        computeResourceGroup: "masterBase_computeResourceGroup_REPLACE_ME"
        availabilitySet: "master_availabilitySet_REPLACE_ME"
        instanceNamePrefix: "masterBase_instanceNamePrefix_REPLACE_ME"
        storageAccountType: "PremiumLRS"
        dataDiskSize: 512
        publicIP: No
    }

    # Config fields for master-1 nodes
    master-1 {
        dataDiskCount: 4
    }

    # Config fields for master-2 nodes
    master-2 {
        dataDiskCount: 3
    }

    # Config fields for worker nodes
    worker {
        computeResourceGroup: "worker_computeResourceGroup_REPLACE_ME"
        availabilitySet: "worker_availabilitySet_REPLACE_ME"
        instanceNamePrefix: "worker_instanceNamePrefix_REPLACE_ME"
        storageAccountType: "StandardLRS"
        dataDiskCount: 11
        dataDiskSize: 1023
        publicIP: No
    }

    # Config fields for edge nodes
    edge {
        computeResourceGroup: "edge_computeResourceGroup_REPLACE_ME"
        availabilitySet: "edge_availabilitySet_REPLACE_ME"
        instanceNamePrefix: "edge_instanceNamePrefix_REPLACE_ME"
        storageAccountType: "PremiumLRS"
        dataDiskCount: 1
        dataDiskSize: 512
        # Change this to Yes to allow accessing edge/CM nodes via public IP
        publicIP: No
    }
}


#
# Bootstrap script
#
# The os-generic bootstrap script will be run after the VM boots up for the first time. This must
# be used to set up preconditions for successful cluster deployment. Director will restart the
# host after the bootstrap script has run.
#
# The example below is an os-generic script that supports these OSes:
#   - CentOS 6.7
#   - CentOS 7.2
#   - RHEL 6.7
#   - RHEL 7.2
#
# The script prepares the OS for cluster installation. It also configures a dhclient or
# NetworkManager hook (depending on OS) to register the A record and PTR record with the DNS server
# configured for the VNET to satisfy proper forward and reverse DNS resolution. Azure's default DNS
# currently does not support Reverse Lookup on private IP Addresses, which is a requirement for
# CDH. See the following link for an example BIND setup to satisfy this requirement:
# http://www.cloudera.com/documentation/director/latest/topics/director_get_started_azure_ddns.html
#
# The scrip also sets required settings for RHEL.
#

bootstrap-script {
    os-generic : """#!/bin/sh

#
# This script will bootstrap these OSes:
#   - CentOS 6.7
#   - CentOS 7.2
#   - RHEL 6.7
#   - RHEL 7.2
#
# Notes and notible differences between OSes:
#   - CentOS 6.7 and RHEL 6.7 use dhclient
#   - CentOS 7.2 and RHEL 7.2 use NetworkManager
#


#
# Functions
#

# writing dhclient-exit-hooks is the same for CentOS 6.7 and RHEL 6.7
# function not indented so EOF works
dhclient_67()
{
# dhclient-exit-hooks explained in dhclient-script man page: http://linux.die.net/man/8/dhclient-script
# cat a here-doc represenation of the hooks to the appropriate file
cat > /etc/dhcp/dhclient-exit-hooks <<"EOF"
#!/bin/bash
printf "\ndhclient-exit-hooks running...\n\treason:%s\n\tinterface:%s\n" "${reason:?}" "${interface:?}"
# only execute on the primary nic
if [ "$interface" != "eth0" ]
then
    exit 0;
fi
# when we have a new IP, perform nsupdate
if [ "$reason" = BOUND ] || [ "$reason" = RENEW ] ||
[ "$reason" = REBIND ] || [ "$reason" = REBOOT ]
then
    printf "\tnew_ip_address:%s\n" "${new_ip_address:?}"
    host=$(hostname -s)
    domain=$(hostname | cut -d'.' -f2- -s)
    domain=${domain:='cdh-cluster.internal'} # If no hostname (see hostFqdnSuffix_REPLACE_ME) is provided, use cdh-cluster.internal
    IFS='.' read -ra ipparts <<< "$new_ip_address"
    ptrrec="$(printf %s "$new_ip_address." | tac -s.)in-addr.arpa"
    nsupdatecmds=$(mktemp -t nsupdate.XXXXXXXXXX)
    resolvconfupdate=$(mktemp -t resolvconfupdate.XXXXXXXXXX)
    echo updating resolv.conf
    grep -iv "search" /etc/resolv.conf > "$resolvconfupdate"
    echo "search $domain" >> "$resolvconfupdate"
    cat "$resolvconfupdate" > /etc/resolv.conf
    echo "Attempting to register $host.$domain and $ptrrec"
    {
        echo "update delete $host.$domain a"
        echo "update add $host.$domain 600 a $new_ip_address"
        echo "send"
        echo "update delete $ptrrec ptr"
        echo "update add $ptrrec 600 ptr $host.$domain"
        echo "send"
    } > "$nsupdatecmds"
    nsupdate "$nsupdatecmds"
fi
#done
exit 0;
EOF
chmod 755 /etc/dhcp/dhclient-exit-hooks
service network restart
}


centos_67()
{
    echo "CentOS 6.7"

    # execute the CentOS 6.7 / RHEL 6.7 dhclient-exit-hooks setup
    dhclient_67
}


rhel_67()
{
    echo "RHEL 6.7"

    # rewrite SELINUX config to disabled and turn off enforcement
    sed -i.bak "s/^SELINUX=.*$/SELINUX=disabled/" /etc/selinux/config
    setenforce 0
    # stop firewall and disable
    service iptables stop
    chkconfig iptables off
    # update config to disable IPv6 and disable
    echo "# Disable IPv6" >> /etc/sysctl.conf
    echo "net.ipv6.conf.all.disable_ipv6 = 1" >> /etc/sysctl.conf
    echo "net.ipv6.conf.default.disable_ipv6 = 1" >> /etc/sysctl.conf
    sysctl -w net.ipv6.conf.all.disable_ipv6=1
    sysctl -w net.ipv6.conf.default.disable_ipv6=1

    # execute the CentOS 6.7 / RHEL 6.7 dhclient-exit-hooks setup
    dhclient_67
}


# writing network manager hooks is the same for CentOS 7.2 and RHEL 7.2
# function not indented so EOF works
networkmanager_72()
{
# Centos 7.2 and RHEL 7.2 uses NetworkManager. Add a script to be automatically invoked when interface comes up.
cat > /etc/NetworkManager/dispatcher.d/12-register-dns <<"EOF"
#!/bin/bash
# NetworkManager Dispatch script
# Deployed by Cloudera Director Bootstrap
#
# Expected arguments:
#    $1 - interface
#    $2 - action
#
# See for info: http://linux.die.net/man/8/networkmanager

# Register A and PTR records when interface comes up
# only execute on the primary nic
if [ "$1" != "eth0" || "$2" != "up" ]
then
    exit 0;
fi

# when we have a new IP, perform nsupdate
new_ip_address="$DHCP4_IP_ADDRESS"

host=$(hostname -s)
domain=$(hostname | cut -d'.' -f2- -s)
domain=${domain:='cdh-cluster.internal'} # If no hostname (see hostFqdnSuffix_REPLACE_ME) is provided, use cdh-cluster.internal
IFS='.' read -ra ipparts <<< "$new_ip_address"
ptrrec="$(printf %s "$new_ip_address." | tac -s.)in-addr.arpa"
nsupdatecmds=$(mktemp -t nsupdate.XXXXXXXXXX)
resolvconfupdate=$(mktemp -t resolvconfupdate.XXXXXXXXXX)
echo updating resolv.conf
grep -iv "search" /etc/resolv.conf > "$resolvconfupdate"
echo "search $domain" >> "$resolvconfupdate"
cat "$resolvconfupdate" > /etc/resolv.conf
echo "Attempting to register $host.$domain and $ptrrec"
{
    echo "update delete $host.$domain a"
    echo "update add $host.$domain 600 a $new_ip_address"
    echo "send"
    echo "update delete $ptrrec ptr"
    echo "update add $ptrrec 600 ptr $host.$domain"
    echo "send"
} > "$nsupdatecmds"
nsupdate "$nsupdatecmds"
exit 0;
EOF
chmod 755 /etc/NetworkManager/dispatcher.d/12-register-dns
service network restart
}


centos_72()
{
    echo "CentOS 7.2"

    # execute the CentOS 7.2 / RHEL 7.2 network manager setup
    networkmanager_72
}


rhel_72()
{
    echo "RHEL 7.2"

    # rewrite SELINUX config to disable and turn off enforcement
    sed -i.bak "s/^SELINUX=.*$/SELINUX=disabled/" /etc/selinux/config
    setenforce 0
    # stop firewall and disable
    systemctl stop iptables
    systemctl iptables off
    # RHEL 7.x uses firewalld
    systemctl stop firewalld
    systemctl disable firewalld
    # Disable tuned so it does not overwrite sysctl.conf
    service tuned stop
    systemctl disable tuned
    # Disable chrony so it does not conflict with ntpd installed by Director
    systemctl stop chronyd
    systemctl disable chronyd
    # update config to disable IPv6 and disable
    echo "# Disable IPv6" >> /etc/sysctl.conf
    echo "net.ipv6.conf.all.disable_ipv6 = 1" >> /etc/sysctl.conf
    echo "net.ipv6.conf.default.disable_ipv6 = 1" >> /etc/sysctl.conf
    # swappniess is set by Director in /etc/sysctl.conf
    # Poke sysctl to have it pickup the config change.
    sysctl -p

    # execute the CentOS 7.2 / RHEL 7.2 network manager setup
    networkmanager_72
}



#
# Main workflow
#

# ensure user is root
if [ "$(id -u)" -ne 0 ]; then
    echo "Please run as root."
    exit 1
fi

# find the OS and release
os=""
release=""

# if it's there, use lsb_release
rpm -q redhat-lsb
if [ $? -eq 0 ]; then
    os=$(lsb_release -si)
    release=$(lsb_release -sr)

# if lsb_release isn't installed, use /etc/redhat-release
else
    grep  "CentOS.* 6\.7" /etc/redhat-release
    if [ $? -eq 0 ]; then
        os="CentOS"
        release="6.7"
    fi

    grep "CentOS.* 7\.2" /etc/redhat-release
    if [ $? -eq 0 ]; then
        os="CentOS"
        release="7.2"
    fi

    grep "Red Hat Enterprise Linux Server release 6.7" /etc/redhat-release
    if [ $? -eq 0 ]; then
        os="RedHatEnterpriseServer"
        release="6.7"
    fi

    grep "Red Hat Enterprise Linux Server release 7.2" /etc/redhat-release
    if [ $? -eq 0 ]; then
        os="RedHatEnterpriseServer"
        release="7.2"
    fi
fi

# debug
echo $os
echo $release


# shellcheck disable=SC2034
not_supported_msg="OS $os $release is not supported."

# select the OS and run the appropriate setup script
if [ "$os" = "CentOS" ]; then
    if [ "$release" = "6.7" ]; then
        centos_67
    elif [ "$release" = "7.2" ]; then
        centos_72
    else
        echo not_supported_msg
        exit 1
    fi

elif [ "$os" = "RedHatEnterpriseServer" ]; then
    if [ "$release" = "6.7" ]; then
        rhel_67
    elif [ "$release" = "7.2" ]; then
        rhel_72
    else
        echo not_supported_msg
        exit 1
    fi
else
    echo not_supported_msg
    exit 1
fi
"""
} # end bootstrap-script


#
# Instant Templates
#

instances {

    master-1 {
        image: ${?common-instanceTemplate.base.image}
        type: ${?common-instanceTemplate.base.type}
        computeResourceGroup: ${?common-instanceTemplate.master-base.computeResourceGroup}
        networkSecurityGroupResourceGroup: ${?common-instanceTemplate.base.networkSecurityGroupResourceGroup}
        networkSecurityGroup: ${?common-instanceTemplate.base.networkSecurityGroup}
        virtualNetworkResourceGroup: ${?common-instanceTemplate.base.virtualNetworkResourceGroup}
        virtualNetwork: ${?common-instanceTemplate.base.virtualNetwork}
        subnetName: ${?common-instanceTemplate.base.subnetName}
        instanceNamePrefix: ${?common-instanceTemplate.master-base.instanceNamePrefix}
        hostFqdnSuffix: ${?common-instanceTemplate.base.hostFqdnSuffix}
        availabilitySet: ${?common-instanceTemplate.master-base.availabilitySet}
        publicIP: ${?common-instanceTemplate.master-base.publicIP}
        storageAccountType: ${?common-instanceTemplate.master-base.storageAccountType}
        dataDiskCount: ${?common-instanceTemplate.master-1.dataDiskCount}
        dataDiskSize: ${?common-instanceTemplate.master-base.dataDiskSize}
        tags: ${?common-instanceTemplate.base.tags}
        bootstrapScript: ${?bootstrap-script.os-generic}
    }

    master-2 {
        image: ${?common-instanceTemplate.base.image}
        type: ${?common-instanceTemplate.base.type}
        computeResourceGroup: ${?common-instanceTemplate.master-base.computeResourceGroup}
        networkSecurityGroupResourceGroup: ${?common-instanceTemplate.base.networkSecurityGroupResourceGroup}
        networkSecurityGroup: ${?common-instanceTemplate.base.networkSecurityGroup}
        virtualNetworkResourceGroup: ${?common-instanceTemplate.base.virtualNetworkResourceGroup}
        virtualNetwork: ${?common-instanceTemplate.base.virtualNetwork}
        subnetName: ${?common-instanceTemplate.base.subnetName}
        instanceNamePrefix: ${?common-instanceTemplate.master-base.instanceNamePrefix}
        hostFqdnSuffix: ${?common-instanceTemplate.base.hostFqdnSuffix}
        availabilitySet: ${?common-instanceTemplate.master-base.availabilitySet}
        publicIP: ${?common-instanceTemplate.master-base.publicIP}
        storageAccountType: ${?common-instanceTemplate.master-base.storageAccountType}
        dataDiskCount: ${?common-instanceTemplate.master-2.dataDiskCount}
        dataDiskSize: ${?common-instanceTemplate.master-base.dataDiskSize}
        tags: ${?common-instanceTemplate.base.tags}
        bootstrapScript: ${?bootstrap-script.os-generic}
    }

    worker {
        image: ${?common-instanceTemplate.base.image}
        type: ${?common-instanceTemplate.base.type}
        computeResourceGroup: ${?common-instanceTemplate.worker.computeResourceGroup}
        networkSecurityGroupResourceGroup: ${?common-instanceTemplate.base.networkSecurityGroupResourceGroup}
        networkSecurityGroup: ${?common-instanceTemplate.base.networkSecurityGroup}
        virtualNetworkResourceGroup: ${?common-instanceTemplate.base.virtualNetworkResourceGroup}
        virtualNetwork: ${?common-instanceTemplate.base.virtualNetwork}
        subnetName: ${?common-instanceTemplate.base.subnetName}
        instanceNamePrefix: ${?common-instanceTemplate.worker.instanceNamePrefix}
        hostFqdnSuffix: ${?common-instanceTemplate.base.hostFqdnSuffix}
        availabilitySet: ${?common-instanceTemplate.worker.availabilitySet}
        publicIP: ${?common-instanceTemplate.worker.publicIP}
        storageAccountType: ${?common-instanceTemplate.worker.storageAccountType}
        dataDiskCount: ${?common-instanceTemplate.worker.dataDiskCount}
        dataDiskSize: ${?common-instanceTemplate.worker.dataDiskSize}
        tags: ${?common-instanceTemplate.base.tags}
        bootstrapScript: ${?bootstrap-script.os-generic}
    }

    edge {
        image: ${?common-instanceTemplate.base.image}
        type: ${?common-instanceTemplate.base.type}
        computeResourceGroup: ${?common-instanceTemplate.edge.computeResourceGroup}
        networkSecurityGroupResourceGroup: ${?common-instanceTemplate.base.networkSecurityGroupResourceGroup}
        networkSecurityGroup: ${?common-instanceTemplate.base.networkSecurityGroup}
        virtualNetworkResourceGroup: ${?common-instanceTemplate.base.virtualNetworkResourceGroup}
        virtualNetwork: ${?common-instanceTemplate.base.virtualNetwork}
        subnetName: ${?common-instanceTemplate.base.subnetName}
        instanceNamePrefix: ${?common-instanceTemplate.edge.instanceNamePrefix}
        hostFqdnSuffix: ${?common-instanceTemplate.base.hostFqdnSuffix}
        availabilitySet: ${?common-instanceTemplate.edge.availabilitySet}
        publicIP: ${?common-instanceTemplate.edge.publicIP}
        storageAccountType: ${?common-instanceTemplate.edge.storageAccountType}
        dataDiskCount: ${?common-instanceTemplate.edge.dataDiskCount}
        dataDiskSize: ${?common-instanceTemplate.edge.dataDiskSize}
        tags: ${?common-instanceTemplate.base.tags}
        bootstrapScript: ${?bootstrap-script.os-generic}
    }

} # End instance templates


#
# Required external database server configuration.
#
# Cloudera Director can create databases on existing database servers.
# NOTE: Cloudera does not support Azure SQL DB service.
#

databaseServers {

    mysqlprod1 {
        type: mysql
        host: "mysqlHost_REPLACE_ME" # Cloudera recommends using the static IP address of database server
        port: 3306
        user: "mysqlUser_REPLACE_ME"
        password: "mysqlPassword_REPLACE_ME"
    }

    # existingpostgres1 {
    #     type: postgresql
    #     host: "pgsqlHost_REPLACE_ME" # Cloudera recommends using the static IP address of database server
    #     port: 5432
    #     user: "pgsqlUser_REPLACE_ME"
    #     password: "pgsqlPassword_REPLACE_ME"
    # }

} # End external database configs


#
# Configuration for Cloudera Manager. Cloudera Director can use an existing Cloudera Manager
# or bootstrap everything from scratch for a new cluster
#

cloudera-manager {

    instance: ${instances.edge} {
        tags {
            application: "Cloudera Manager 5"
        }
    }

    #
    # Licensing configuration
    #
    # There are three mutually exclusive options for setting up Cloudera Manager's license.
    # 1. License text may be embedded in this file using the "license" field. Triple quotes (""")
    #    are recommended for including multi-line text strings.
    # 2. The "licensePath" can be used to specify the path to a file containing the license.
    # 3. The "enableEnterpriseTrial" flag indicates whether the 60-Day Cloudera Enterprise Trial
    #    should be activated when no license is present. This must not be set to true if a
    #    license is included using either "license" or "licensePath".

    #
    # Embed a license for Cloudera Manager
    #

    # license: """
    #   -----BEGIN PGP SIGNED MESSAGE-----
    #   Hash: SHA1
    #
    # {
    #   "version"        : 1,
    #   "name"           : "License Owner",
    #   "uuid"           : "license id",
    #   "expirationDate" : 0,
    #   "features"       : [ "FEATURE1", "FEATURE2" ]
    # }
    # -----BEGIN PGP SIGNATURE-----
    # Version: GnuPG v1.4.11 (GNU/Linux)
    #
    # PGP SIGNATURE
    # -----END PGP SIGNATURE-----
    # """

    #
    # Include a license for Cloudera Manager from an external file
    #

    # licensePath: "/path/to/license.txt.asc"

    #
    # Specify the billingId.
    #
    # Cloudera Director will use the billing ID to report usage information to a metering service
    # for usage based billing.
    #
    # Usage reporting starts as soon as you assign a billing ID and a license to a Cloudera Manager.
    # If you remove a billing ID, Director will stop reporting to the metering service.
    #
    # When usage reporting stops, you will not have access to Cloudera Support with this deployment.
    # If you want a billing ID, please contact Cloudera.
    #

    # billingId: "billingId_REPLACE_ME"

    #
    # Activate 60-Day Cloudera Enterprise Trial
    #

    enableEnterpriseTrial: true

    #
    # Install the unlimited strength JCE policy files for higher levels of encryption.
    # Prior to setting this to true, confirm that you understand the legal ramifications
    # of using unlimited JCE policy files in your country.
    #

    unlimitedJce: true

    # An administrative Kerberos account capable of creating principals on the KDC that
    # Cloudera Manager will be using. This will typically be in the format:
    #    Principal@YOUR.KDC.REALM
    krbAdminUsername: "krbAdminUsername_REPLACE_ME"

    # The password for the administrative Kerberos account.
    krbAdminPassword: "krbAdminPassword_REPLACE_ME"


    #
    # Optional database configuration
    #
    # There are three mutually exclusive options for database usage in Cloudera Director.
    # 1. This option is NOT supported for production use.
    #    With no configuration, an embedded PostgreSQL database will be used.
    # 2. Alternatively, existing external databases can be used.
    # 3. Finally, databases can be created on the fly on existing external database servers.

    #
    # Optional configuration for existing external databases
    #
    # databases {
    #     CLOUDERA_MANAGER {
    #         type: postgresql
    #
    #         host: db.example.com
    #         port: 123
    #
    #         user: admin
    #         password: 1231ed
    #
    #         name: scm
    #     }
    #
    #     ACTIVITYMONITOR { ... }
    #
    #     REPORTSMANAGER { ... }
    #
    #     NAVIGATOR { ... }
    #
    #     # Added in Cloudera Manager 5.2+
    #     NAVIGATORMETASERVER { ... }
    # }

    #
    # Optional configuration for creating external databases on the fly
    #
    # When a database is created on the fly, Director generates a random database name using the specified database
    # name prefix, a random username based on the specified username prefix, and a random password. The password is
    # stored by Director and made available to the service that uses the database. If multiple services reference the
    # same external database server, Director will create a database for each.
    #
    # MySQL limits usernames to sixteen characters. Therefore, limit usernamePrefix values for databases on MySQL to
    # seven characters; the remaining nine characters are used by the randomized suffix generated by Director.
    #

    databaseTemplates {
        CLOUDERA_MANAGER {
            name: cmtemplate
            databaseServerName: mysqlprod1 # Must correspond to an external database server named above
            databaseNamePrefix: scm
            usernamePrefix: cmadmin
        }

        ACTIVITYMONITOR {
            name: amontemplate
            databaseServerName: mysqlprod1 # Must correspond to an external database server named above
            databaseNamePrefix: amon
            usernamePrefix: amadmin
        }

        REPORTSMANAGER {
            name: rmantemplate
            databaseServerName: mysqlprod1 # Must correspond to an external database server named above
            databaseNamePrefix: rman
            usernamePrefix: rmadmin
        }

        NAVIGATOR {
            name: navtemplate
            databaseServerName: mysqlprod1 # Must correspond to an external database server named above
            databaseNamePrefix: nav
            usernamePrefix: nadmin
        }

        # Added in Cloudera Manager 5.2+
        NAVIGATORMETASERVER {
            name: navmetatemplate
            databaseServerName: mysqlprod1 # Must correspond to an external database server named above
            databaseNamePrefix: navmeta
            usernamePrefix: nmadmin
        }
    }

    #
    # Configuration to override Cloudera Manager package repositories
    #

    # repository: "http://archive.cloudera.com/cm5/redhat/6/x86_64/cm/5.9/"
    # repositoryKeyUrl: "http://archive.cloudera.com/cm5/redhat/6/x86_64/cm/RPM-GPG-KEY-cloudera"

    # OR use an existing Cloudera Manager installation

    # hostname: "192.168.33.10"
    # username: <if not default 'admin'>
    # password: <if not default 'admin'>

    #
    # Optional configuration for Cloudera Manager and its management services
    #
    # Configuration properties for CLOUDERA_MANAGER are documented at
    # http://www.cloudera.com/content/cloudera/en/documentation/core/latest/topics/cm_props_cmserver.html
    #
    # Configuration properties for the Cloudera Management services are documented at
    # http://www.cloudera.com/content/cloudera/en/documentation/core/latest/topics/cm_props_mgmtservice.html
    #
    # Configuration properties for Hosts are documented at
    # http://www.cloudera.com/content/cloudera/en/documentation/core/latest/topics/cm_props_host.html
    #
    configs {
        # CLOUDERA_MANAGER corresponds to the Cloudera Manager Server configuration options
        CLOUDERA_MANAGER {
            # enable_api_debug: false
            custom_banner_html: "Managed by Cloudera Director"
            # Kerberos Settings:

            # The type of KDC Cloudera Manager will be using. Valid values are "MIT KDC"
            # and "Active Directory"
            KDC_TYPE: "MIT KDC"

            # The KDC host name or IP address.
            KDC_HOST: "KDC_HOST_REPLACE_ME"

            # The security realm that your KDC uses. This will be of the format of a fully
            # qualified domain name:
            #    YOUR.KDC.REALM
            SECURITY_REALM: "SECURITY_REALM_REPLACE_ME"

            # The Active Directory KDC domain. Only applicable to Active Directory KDCs. This
            # will be in the format of an X.500 Directory Specification:
            #    DC=domain,DC=example,DC=com
            # AD_KDC_DOMAIN: "AD_KDC_DOMAIN_REPLACE_ME"

            # Allow Cloudera Manager to deploy Kerberos configurations to hosts. This should
            # be set to true unless you have an alternate mechanism to generate or retrieve the
            # Kerberos configuration on your Cloudera Manager node instances.
            KRB_MANAGE_KRB5_CONF: true

            # The encryption types your KDC supports. Some of those listed below will require the
            # unlimited strength JCE policy files.
            KRB_ENC_TYPES: "aes256-cts aes128-cts des3-hmac-sha1 arcfour-hmac des-hmac-sha1 des-cbc-md5 des-cbc-crc"

            # There are many more optional Kerberos configuration options available to Cloudera Manager.
            # Please refer to the Kerberos section on
            # http://www.cloudera.com/content/cloudera/en/documentation/core/latest/topics/cm_props_cmserver.html
            # for more details.
        }

        # CLOUDERA_MANAGEMENT_SERVICE corresponds to the Service-Wide configuration options
        CLOUDERA_MANAGEMENT_SERVICE {
            # enable_alerts : false
            # enable_config_alerts : false
        }

        SERVICEMONITOR {
            mgmt_log_dir:/data0/log/cloudera-scm-firehose
            firehose_storage_dir:/data0/lib/cloudera-service-monitor
        }

        ACTIVITYMONITOR {
            mgmt_log_dir:/data0/log/cloudera-scm-firehose
        }

        HOSTMONITOR {
            mgmt_log_dir: /data0/log/cloudera-scm-firehose
            firehose_storage_dir: /data0/lib/cloudera-host-monitor
        }

        REPORTSMANAGER {
            headlamp_scratch_dir: /data0/lib/cloudera-scm-headlamp
            mgmt_log_dir: /data0/log/cloudera-scm-headlamp
        }

        EVENTSERVER {
            mgmt_log_dir:/data0/log/cloudera-scm-eventserver
            eventserver_index_dir:/data0/lib/cloudera-scm-eventserver
        }

        ALERTPUBLISHER {
            mgmt_log_dir:/data0/log/cloudera-scm-alertpublisher
        }

        NAVIGATOR {
            mgmt_log_dir:/data0/log/cloudera-scm-navigator
        }

        NAVIGATORMETASERVER {
            audit_event_log_dir:/data0/log/cloudera-scm-navigator/audit
            data_dir:/data0/lib/cloudera-scm-navigator
            mgmt_log_dir:/data0/log/cloudera-scm-navigator
        }

        # Configuration properties for all hosts
        HOSTS {
        }
    }
} # End CM configuration

#
# Highly Available Cluster description
#

cluster {

    # List the products and their versions that need to be installed.
    # These products must have a corresponding parcel in the parcelRepositories
    # configured above. The specified version will be used to find a suitable
    # parcel. Specifying a version that points to more than one parcel among
    # those available will result in a configuration error. Specify more granular
    # versions to avoid conflicts.

    products {
        CDH: 5
    }

    #
    # Optional override of CDH parcel repositories
    #

    # parcelRepositories: ["http://archive.cloudera.com/cdh5/parcels/5.9/"]

    services: [HDFS, YARN, ZOOKEEPER, HBASE, HIVE, HUE, IMPALA, OOZIE, SPARK_ON_YARN]

    #
    # Optional custom service configurations
    # Configuration keys containing periods must be enclosed in double quotes.
    #

    configs {
        # HDFS fencing should be set to true for HA configurations
        HDFS {
            dfs_ha_fencing_methods: "shell(true)"
            dfs_replication: "3"
            dfs_block_local_path_access_user: "impala,hbase,mapred,spark"
        }

        # Configuring OOZIE high availability (optional) requires a load balancer to be specified
        # Director does not create or manage the load balancer.
        #
        # The load balancer must be configured with the IPs of the oozie servers
        # after the cluster completes bootstrapping.
        # OOZIE {
        #     oozie_load_balancer: "oozie_load_balancer_REPLACE_ME:11000"
        # }

        HIVE {
            audit_event_log_dir: /data0/log/hive/audit
            lineage_event_log_dir: /data0/log/hive/lineage
        }

        HBASE {
            audit_event_log_dir: /data0/log/hbase/audit
        }
    }

    #
    # High availibility configuration requires external databases to be defined for the
    # Hive Metastore, Hue, and Oozie services. These databases may be configured either
    # as existing external databases using the "databases" block below, or as "databaseTemplate"
    # to be created on a databaseServer.
    #

    #
    # Optional configuration for existing external database for Hive Metastore, Hue,
    # and Oozie databases
    #

    # databases {
    #     HIVE {
    #         type: postgresql
    #         host: db.example.com
    #         port: 123
    #         user: hive
    #         password: pass
    #         name: hive_db
    #     }
    #     HUE {
    #         type: postgresql
    #         host: db.example.com
    #         port: 123
    #         user: hue
    #         password: pass
    #         name: hue_db
    #     }
    #     OOZIE {
    #         type: postgresql
    #         host: db.example.com
    #         port: 123
    #         user: oozie
    #         password: pass
    #         name: oozie_db
    #     }
    # }

    #
    # Optional configuration for creating external database on the fly for Hive Metastore
    # Hue, and Oozie databases
    #

    databaseTemplates: {
        HIVE {
            name: hivetemplate
            databaseServerName: mysqlprod1 # Must correspond to an external database server named above
            databaseNamePrefix: hivemetastore
            usernamePrefix: hive
        }

        HUE {
            name: huetemplate
            databaseServerName: mysqlprod1 # Must correspond to an external database server named above
            databaseNamePrefix: huedb
            usernamePrefix: hue
        }

        OOZIE {
            name: oozietemplate
            databaseServerName: mysqlprod1 # Must correspond to an external database server named above
            databaseNamePrefix: ooziedb
            usernamePrefix: oozie
        }
    }

    #
    # This reference configuration follows the Cloudera Azure Reference Architecture.
    #

    masters-1 {
        count: 2

        instance: ${instances.master-1} {
            tags {
                group: masters-1
            }
        }

        roles {
            ZOOKEEPER: [SERVER]
            HDFS: [NAMENODE, FAILOVERCONTROLLER, JOURNALNODE]
            YARN: [RESOURCEMANAGER]
            HBASE: [MASTER]
        }

        # NameNode nameservice, autofailover, and quorum journal name must be configured for high availability
        configs {
            HDFS {
                NAMENODE {
                    dfs_federation_namenode_nameservice: hanameservice
                    autofailover_enabled: true
                    dfs_namenode_quorum_journal_name: hanameservice

                    namenode_log_dir: /data0/log/hadoop-hdfs
                    dfs_name_dir_list: /data3/dfs/nn
                }
                FAILOVERCONTROLLER
                {
                    failover_controller_log_dir: /data0/log/hadoop-hdfs
                }
                JOURNALNODE
                {
                  journalnode_log_dir: /data0/log/hadoop-hdfs
                  dfs_journalnode_edits_dir: /data1/hdfs
                }
            }
            ZOOKEEPER {
                SERVER {
                    zk_server_log_dir: /data0/log/zookeeper
                    dataDir: /data2/zookeeper
                    dataLogDir: /data2/zookeeper
                    maxClientCnxns: 1024
                }
            }
            YARN {
              RESOURCEMANAGER {
                resource_manager_log_dir: /data0/log/hadoop-yarn
              }
            }
            HBASE {
              MASTER {
                hbase_master_log_dir: /data0/log/hbase
              }
            }
        }
    }

    masters-2 {
        count: 1

        instance: ${instances.master-2} {
            tags {
                group: masters-2
            }
        }

        roles {
            ZOOKEEPER: [SERVER]
            HDFS: [JOURNALNODE, HTTPFS]
            HIVE: [HIVESERVER2, HIVEMETASTORE, WEBHCAT]
            YARN: [JOBHISTORY]
            HUE: [HUE_SERVER]
            OOZIE: [OOZIE_SERVER]
            IMPALA: [CATALOGSERVER, STATESTORE]
            SPARK_ON_YARN: [SPARK_YARN_HISTORY_SERVER]
            HBASE: [HBASETHRIFTSERVER] # Alternately [HBASERESTSERVER], for HUE Integration
        }

        # Oozie plugins must be configured for high availability
        configs {
            HDFS {
                JOURNALNODE
                {
                    journalnode_log_dir: /data0/log/hadoop-hdfs
                    dfs_journalnode_edits_dir: /data1/hdfs
                }
                HTTPFS
                {
                    httpfs_log_dir: /data0/log/hadoop-httpfs
                }
            }
            OOZIE {
                OOZIE_SERVER {
                    oozie_plugins_list: "org.apache.oozie.service.ZKLocksService,org.apache.oozie.service.ZKXLogStreamingService,org.apache.oozie.service.ZKJobsConcurrencyService,org.apache.oozie.service.ZKUUIDService"
                    oozie_log_dir: /data0/log/oozie
                }
            }
            ZOOKEEPER {
                SERVER {
                    zk_server_log_dir: /data0/log/zookeeper
                    dataDir: /data2/zookeeper
                    dataLogDir: /data2/zookeeper
                    maxClientCnxns: 1024
                }
            }
            HIVE {
                HIVEMETASTORE {
                    hive_log_dir: /data0/log/hive
                }
                HIVESERVER2 {
                    hive_log_dir: /data0/log/hive
                }
                WEBHCAT {
                    hcatalog_log_dir: /data0/log/hcatalog
                }
            }
            YARN {
                JOBHISTORY {
                    mr2_jobhistory_log_dir: /data0/log/hadoop-mapreduce
                }
            }
            HUE {
                HUE_SERVER {
                    hue_server_log_dir: /data0/log/hue
                }
                KT_RENEWER {
                    kt_renewer_log_dir: /data0/log/hue
                }
            }
            IMPALA {
                CATALOGSERVER {
                    log_dir: /data0/log/catalogd
                }
                STATESTORE {
                    log_dir: /data0/log/statestore
                }
            }
            SPARK_ON_YARN {
                SPARK_YARN_HISTORY_SERVER {
                    log_dir: /data0/log/spark
                }
            }
            HBASE {
                HBASETHRIFTSERVER {
                    hbase_thriftserver_log_dir: /data0/log/hbase
                }
                #HBASERESTSERVER {
                #    hbase_restserver_log_dir: /data0/log/hbase
                #}
            }
        }
    }

    workers {
        #
        # The desired number of instances to provision. Cloudera Director will attempt to allocate
        # this many instances but will not fail the deployment as long as the minimum number of
        # instances (specified with minCount below) is allocated. If minCount is not specified
        # then minCount is set to count.
        #
        count: 5

        #
        # Minimum number of instances required to set up the cluster.
        # Fail and quit if minCount number of instances is not available in this cloud
        # environment. Else, continue setting up the cluster. If minCount is not defined then
        # minCount defaults to count.
        #
        minCount: 5

        instance: ${instances.worker} {
            tags {
                group: worker
            }
        }

        roles {
            HDFS: [DATANODE]
            YARN: [NODEMANAGER]
            HBASE: [REGIONSERVER]
            IMPALA: [IMPALAD]
        }

        # Optional custom role configurations
        # Configuration keys containing periods must be enclosed in double quotes.
        configs {
            HDFS {
                DATANODE {
                    datanode_log_dir: /data0/log/hadoop-hdfs
                    dfs_data_dir_list: "/data1/dfs/dn,/data2/dfs/dn,/data3/dfs/dn,/data4/dfs/dn,/data5/dfs/dn,/data6/dfs/dn,/data7/dfs/dn,/data8/dfs/dn,/data9/dfs/dn,/data10/dfs/dn"
                    dfs_datanode_failed_volumes_tolerated: 1
                }
            }
            YARN {
                NODEMANAGER {
                    node_manager_log_dir: /data0/log/hadoop-yarn
                    yarn_nodemanager_log_dirs: "/data1/log/hadoop-yarn/container,/data2/log/hadoop-yarn/container,/data3/log/hadoop-yarn/container,/data4/log/hadoop-yarn/container,/data5/log/hadoop-yarn/container,/data6/log/hadoop-yarn/container,/data7/log/hadoop-yarn/container,/data8/log/hadoop-yarn/container,/data9/log/hadoop-yarn/container,/data10/log/hadoop-yarn/container"
                    yarn_nodemanager_local_dirs: "/data1/yarn,/data2/yarn,/data3/yarn,/data4/yarn,/data5/yarn,/data6/yarn,/data7/yarn,/data8/yarn,/data9/yarn,/data10/yarn"
                }
            }
            HBASE {
                REGIONSERVER {
                    hbase_regionserver_log_dir: /data0/log/hbase
                }
            }
            IMPALA {
                IMPALAD {
                    log_dir: /data0/log/impalad
                    lineage_event_log_dir: /data0/log/impalad/lineage
                    audit_event_log_dir: /data0/log/impalad/audit
                    scratch_dirs: "/data1/impala/impalad,/data2/impala/impalad,/data3/impala/impalad,/data4/impala/impalad,/data5/impala/impalad,/data6/impala/impalad,/data7/impala/impalad,/data8/impala/impalad,/data9/impala/impalad,/data10/impala/impalad"
                }
            }
        }
    }

    postCreateScripts: ["""#!/bin/sh

# This is an embedded post creation script that runs as root and can be used to
# customize the cluster after it has been created.

# If the exit code is not zero Cloudera Director will fail

# Post creation scripts also have access to the following environment variables:

#    DEPLOYMENT_HOST_PORT
#    ENVIRONMENT_NAME
#    DEPLOYMENT_NAME
#    CLUSTER_NAME
#    CM_USERNAME
#    CM_PASSWORD

echo 'Hello World!'
exit 0
    """,
    """#!/usr/bin/python

# Additionally, multiple post-creation scripts can be supplied.  They will run
# in the order they are listed here.  Interpeters other than bash can be used
# as well.

print 'Hello again!'
    """]

    # For more complex scripts, post creation scripts can be supplied via path,
    # where they will be read from the local filesystem.  They will run after
    # any scripts supplied in the previous postCreateScripts section.
    # postCreateScriptsPaths: ["/tmp/test-script.sh",
    #                         "/tmp/test-script.py"]

    preTerminateScripts: ["""#!/bin/sh

# This is an embedded pre-termination script that runs as root and can be used to
# customize the cluster after it has been created.

# If the exit code is not zero Cloudera Director will fail

# Pre terminate scripts also have access to the following environment variables:

#    DEPLOYMENT_HOST_PORT
#    ENVIRONMENT_NAME
#    DEPLOYMENT_NAME
#    CLUSTER_NAME
#    CM_USERNAME
#    CM_PASSWORD

echo 'Goodbye World!'
exit 0
    """,
    """#!/usr/bin/python

# Additionally, multiple pre terminate scripts can be supplied.  They will run
# in the order they are listed here.  Interpeters other than bash can be used
# as well.

print 'Goodbye again!'
        """]

    # For more complex scripts, pre terminate scripts can be supplied via path,
    # where they will be read from the local filesystem.  They will run after
    # any scripts supplied in the previous preTerminateScripts section.
    # preTerminateScriptsPaths: ["/tmp/test-script.sh",
    #                            "/tmp/test-script.py"]
}
